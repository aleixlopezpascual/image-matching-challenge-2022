{
 "metadata": {
  "kernelspec": {
   "name": "image-matching",
   "language": "python",
   "display_name": "image-matching"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install ./input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n",
    "!pip install ./input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n",
    "!pip install ./input/loftrutils/einops-0.4.1-py3-none-any.whl"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:07:47.120151Z",
     "iopub.execute_input": "2022-04-17T18:07:47.120937Z",
     "iopub.status.idle": "2022-04-17T18:07:47.162885Z",
     "shell.execute_reply.started": "2022-04-17T18:07:47.120898Z",
     "shell.execute_reply": "2022-04-17T18:07:47.162108Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://aleix.lopez%40glovoapp.com:****@artifactory.glovoint.com/artifactory/api/pypi/glovo-pypi/simple\r\n",
      "Processing ./input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: torch>=1.8.1 in ./.venv/lib/python3.8/site-packages (from kornia==0.6.4) (1.11.0)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from kornia==0.6.4) (21.3)\r\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.8/site-packages (from torch>=1.8.1->kornia==0.6.4) (4.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.venv/lib/python3.8/site-packages (from packaging->kornia==0.6.4) (3.0.9)\r\n",
      "kornia is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1.1 is available.\r\n",
      "You should consider upgrading via the '/Users/aleix.lopezglovoapp.com/image-matching-challenge-2022/.venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Looking in indexes: https://aleix.lopez%40glovoapp.com:****@artifactory.glovoint.com/artifactory/api/pypi/glovo-pypi/simple\r\n",
      "Processing ./input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\r\n",
      "Requirement already satisfied: kornia in ./.venv/lib/python3.8/site-packages (from kornia-moons==0.1.9) (0.6.4)\r\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.8/site-packages (from kornia-moons==0.1.9) (4.5.5.64)\r\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.8/site-packages (from kornia-moons==0.1.9) (1.11.0)\r\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (from kornia-moons==0.1.9) (3.5.2)\r\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from kornia->kornia-moons==0.1.9) (21.3)\r\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.8/site-packages (from torch->kornia-moons==0.1.9) (4.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->kornia-moons==0.1.9) (3.0.9)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->kornia-moons==0.1.9) (4.33.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->kornia-moons==0.1.9) (1.4.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->kornia-moons==0.1.9) (9.1.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib->kornia-moons==0.1.9) (0.11.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.8/site-packages (from matplotlib->kornia-moons==0.1.9) (1.22.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib->kornia-moons==0.1.9) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->kornia-moons==0.1.9) (1.16.0)\r\n",
      "kornia-moons is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1.1 is available.\r\n",
      "You should consider upgrading via the '/Users/aleix.lopezglovoapp.com/image-matching-challenge-2022/.venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Looking in indexes: https://aleix.lopez%40glovoapp.com:****@artifactory.glovoint.com/artifactory/api/pypi/glovo-pypi/simple\r\n",
      "Processing ./input/loftrutils/einops-0.4.1-py3-none-any.whl\r\n",
      "einops is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1.1 is available.\r\n",
      "You should consider upgrading via the '/Users/aleix.lopezglovoapp.com/image-matching-challenge-2022/.venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://aleix.lopez%40glovoapp.com:****@artifactory.glovoint.com/artifactory/api/pypi/glovo-pypi/simple\r\n",
      "Requirement already satisfied: loguru in ./.venv/lib/python3.8/site-packages (0.6.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.1.1 is available.\r\n",
      "You should consider upgrading via the '/Users/aleix.lopezglovoapp.com/image-matching-challenge-2022/.venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import gc\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from loguru import logger\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./input/loftrutils/LoFTR-master/LoFTR-master/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from src.loftr import LoFTR\n",
    "from src.loftr.utils.supervision import (\n",
    "    compute_supervision_coarse,\n",
    "    compute_supervision_fine,\n",
    ")\n",
    "from src.losses.loftr_loss import LoFTRLoss\n",
    "from src.optimizers import build_optimizer, build_scheduler\n",
    "from src.utils.comm import all_gather, gather\n",
    "from src.utils.metrics import (\n",
    "    aggregate_metrics,\n",
    "    compute_pose_errors,\n",
    "    compute_symmetrical_epipolar_errors,\n",
    ")\n",
    "from src.utils.misc import flattenList, lower_config\n",
    "from src.utils.plotting import make_matching_figures\n",
    "from src.utils.profiler import PassThroughProfiler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class PL_LoFTR(pl.LightningModule):\n",
    "    def __init__(self, config, pretrained_ckpt=None, profiler=None, dump_dir=None):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            - use the new version of PL logging API.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Misc\n",
    "        self.config = config  # full config\n",
    "        _config = lower_config(self.config)\n",
    "        self.loftr_cfg = lower_config(_config[\"loftr\"])\n",
    "        self.profiler = profiler or PassThroughProfiler()\n",
    "        self.n_vals_plot = (\n",
    "            1  # max(config.TRAINER.N_VAL_PAIRS_TO_PLOT // config.TRAINER.WORLD_SIZE, 1)\n",
    "        )\n",
    "\n",
    "        # Matcher: LoFTR\n",
    "        self.matcher = LoFTR(config=_config[\"loftr\"])\n",
    "        self.loss = LoFTRLoss(_config)\n",
    "\n",
    "        # Pretrained weights\n",
    "        if pretrained_ckpt:\n",
    "            state_dict = torch.load(pretrained_ckpt, map_location=\"cuda\")[\"state_dict\"]\n",
    "            self.matcher.load_state_dict(state_dict, strict=True)\n",
    "            logger.info(f\"Load '{pretrained_ckpt}' as pretrained checkpoint\")\n",
    "\n",
    "        # Testing\n",
    "        self.dump_dir = dump_dir\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # FIXME: The scheduler did not work properly when `--resume_from_checkpoint`\n",
    "        optimizer = build_optimizer(self, self.config)\n",
    "        scheduler = build_scheduler(self.config, optimizer)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def optimizer_step(\n",
    "        self,\n",
    "        epoch,\n",
    "        batch_idx,\n",
    "        optimizer,\n",
    "        optimizer_idx,\n",
    "        optimizer_closure,\n",
    "        on_tpu,\n",
    "        using_native_amp,\n",
    "        using_lbfgs,\n",
    "    ):\n",
    "        # learning rate warm up\n",
    "        warmup_step = self.config.TRAINER.WARMUP_STEP\n",
    "        if self.trainer.global_step < warmup_step:\n",
    "            if self.config.TRAINER.WARMUP_TYPE == \"linear\":\n",
    "                base_lr = self.config.TRAINER.WARMUP_RATIO * self.config.TRAINER.TRUE_LR\n",
    "                lr = base_lr + (\n",
    "                    self.trainer.global_step / self.config.TRAINER.WARMUP_STEP\n",
    "                ) * abs(self.config.TRAINER.TRUE_LR - base_lr)\n",
    "                for pg in optimizer.param_groups:\n",
    "                    pg[\"lr\"] = lr\n",
    "            elif self.config.TRAINER.WARMUP_TYPE == \"constant\":\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unknown lr warm-up strategy: {self.config.TRAINER.WARMUP_TYPE}\"\n",
    "                )\n",
    "\n",
    "        # update params\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    def _trainval_inference(self, batch):\n",
    "        with self.profiler.profile(\"Compute coarse supervision\"):\n",
    "            compute_supervision_coarse(batch, self.config)\n",
    "\n",
    "        with self.profiler.profile(\"LoFTR\"):\n",
    "            self.matcher(batch)\n",
    "\n",
    "        with self.profiler.profile(\"Compute fine supervision\"):\n",
    "            compute_supervision_fine(batch, self.config)\n",
    "\n",
    "        with self.profiler.profile(\"Compute losses\"):\n",
    "            self.loss(batch)\n",
    "\n",
    "    def _compute_metrics(self, batch):\n",
    "        with self.profiler.profile(\"Copmute metrics\"):\n",
    "            compute_symmetrical_epipolar_errors(\n",
    "                batch\n",
    "            )  # compute epi_errs for each match\n",
    "            compute_pose_errors(\n",
    "                batch, self.config\n",
    "            )  # compute R_errs, t_errs, pose_errs for each pair\n",
    "\n",
    "            rel_pair_names = list(zip(*batch[\"pair_names\"]))\n",
    "            bs = batch[\"image0\"].size(0)\n",
    "            metrics = {\n",
    "                # to filter duplicate pairs caused by DistributedSampler\n",
    "                \"identifiers\": [\"#\".join(rel_pair_names[b]) for b in range(bs)],\n",
    "                \"epi_errs\": [\n",
    "                    batch[\"epi_errs\"][batch[\"m_bids\"] == b].cpu().numpy()\n",
    "                    for b in range(bs)\n",
    "                ],\n",
    "                \"R_errs\": batch[\"R_errs\"],\n",
    "                \"t_errs\": batch[\"t_errs\"],\n",
    "                \"inliers\": batch[\"inliers\"],\n",
    "            }\n",
    "            ret_dict = {\"metrics\": metrics}\n",
    "        return ret_dict, rel_pair_names\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self._trainval_inference(batch)\n",
    "\n",
    "        # logging\n",
    "        if (\n",
    "            self.trainer.global_rank == 0\n",
    "            and self.global_step % self.trainer.log_every_n_steps == 0\n",
    "        ):\n",
    "            # scalars\n",
    "            for k, v in batch[\"loss_scalars\"].items():\n",
    "                self.logger.experiment.add_scalar(f\"train/{k}\", v, self.global_step)\n",
    "\n",
    "            # net-params\n",
    "            if self.config.LOFTR.MATCH_COARSE.MATCH_TYPE == \"sinkhorn\":\n",
    "                self.logger.experiment.add_scalar(\n",
    "                    f\"skh_bin_score\",\n",
    "                    self.matcher.coarse_matching.bin_score.clone().detach().cpu().data,\n",
    "                    self.global_step,\n",
    "                )\n",
    "\n",
    "            # figures\n",
    "            if self.config.TRAINER.ENABLE_PLOTTING:\n",
    "                compute_symmetrical_epipolar_errors(\n",
    "                    batch\n",
    "                )  # compute epi_errs for each match\n",
    "                figures = make_matching_figures(\n",
    "                    batch, self.config, self.config.TRAINER.PLOT_MODE\n",
    "                )\n",
    "                for k, v in figures.items():\n",
    "                    self.logger.experiment.add_figure(\n",
    "                        f\"train_match/{k}\", v, self.global_step\n",
    "                    )\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return {\"loss\": batch[\"loss\"]}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        if self.trainer.global_rank == 0:\n",
    "            self.logger.experiment.add_scalar(\n",
    "                \"train/avg_loss_on_epoch\", avg_loss, global_step=self.current_epoch\n",
    "            )\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._trainval_inference(batch)\n",
    "\n",
    "        ret_dict, _ = self._compute_metrics(batch)\n",
    "\n",
    "        val_plot_interval = max(self.trainer.num_val_batches[0] // self.n_vals_plot, 1)\n",
    "        figures = {self.config.TRAINER.PLOT_MODE: []}\n",
    "        if batch_idx % val_plot_interval == 0:\n",
    "            figures = make_matching_figures(\n",
    "                batch, self.config, mode=self.config.TRAINER.PLOT_MODE\n",
    "            )\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return {\n",
    "            **ret_dict,\n",
    "            \"loss_scalars\": batch[\"loss_scalars\"],\n",
    "            \"figures\": figures,\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # handle multiple validation sets\n",
    "        multi_outputs = (\n",
    "            [outputs] if not isinstance(outputs[0], (list, tuple)) else outputs\n",
    "        )\n",
    "        multi_val_metrics = defaultdict(list)\n",
    "\n",
    "        for valset_idx, outputs in enumerate(multi_outputs):\n",
    "            # since pl performs sanity_check at the very begining of the training\n",
    "            cur_epoch = self.trainer.current_epoch\n",
    "            if not self.trainer.resume_from_checkpoint:\n",
    "                cur_epoch = -1\n",
    "\n",
    "            # 1. loss_scalars: dict of list, on cpu\n",
    "            _loss_scalars = [o[\"loss_scalars\"] for o in outputs]\n",
    "            loss_scalars = {\n",
    "                k: flattenList(all_gather([_ls[k] for _ls in _loss_scalars]))\n",
    "                for k in _loss_scalars[0]\n",
    "            }\n",
    "\n",
    "            # 2. val metrics: dict of list, numpy\n",
    "            _metrics = [o[\"metrics\"] for o in outputs]\n",
    "            metrics = {\n",
    "                k: flattenList(all_gather(flattenList([_me[k] for _me in _metrics])))\n",
    "                for k in _metrics[0]\n",
    "            }\n",
    "            # NOTE: all ranks need to `aggregate_merics`, but only log at rank-0\n",
    "            val_metrics_4tb = aggregate_metrics(\n",
    "                metrics, self.config.TRAINER.EPI_ERR_THR\n",
    "            )\n",
    "            for thr in [5, 10, 20]:\n",
    "                multi_val_metrics[f\"auc@{thr}\"].append(val_metrics_4tb[f\"auc@{thr}\"])\n",
    "\n",
    "            # 3. figures\n",
    "            _figures = [o[\"figures\"] for o in outputs]\n",
    "            figures = {\n",
    "                k: flattenList(gather(flattenList([_me[k] for _me in _figures])))\n",
    "                for k in _figures[0]\n",
    "            }\n",
    "\n",
    "            # tensorboard records only on rank 0\n",
    "            if self.trainer.global_rank == 0:\n",
    "                for k, v in loss_scalars.items():\n",
    "                    mean_v = torch.stack(v).mean()\n",
    "                    self.logger.experiment.add_scalar(\n",
    "                        f\"val_{valset_idx}/avg_{k}\", mean_v, global_step=cur_epoch\n",
    "                    )\n",
    "\n",
    "                for k, v in val_metrics_4tb.items():\n",
    "                    self.logger.experiment.add_scalar(\n",
    "                        f\"metrics_{valset_idx}/{k}\", v, global_step=cur_epoch\n",
    "                    )\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            plt.close(\"all\")\n",
    "\n",
    "        for thr in [5, 10, 20]:\n",
    "            # log on all ranks for ModelCheckpoint callback to work properly\n",
    "            self.log(\n",
    "                f\"auc@{thr}\", torch.tensor(np.mean(multi_val_metrics[f\"auc@{thr}\"]))\n",
    "            )  # ckpt monitors on this\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        with self.profiler.profile(\"LoFTR\"):\n",
    "            self.matcher(batch)\n",
    "\n",
    "        ret_dict, rel_pair_names = self._compute_metrics(batch)\n",
    "\n",
    "        with self.profiler.profile(\"dump_results\"):\n",
    "            if self.dump_dir is not None:\n",
    "                # dump results for further analysis\n",
    "                keys_to_save = {\"mkpts0_f\", \"mkpts1_f\", \"mconf\", \"epi_errs\"}\n",
    "                pair_names = list(zip(*batch[\"pair_names\"]))\n",
    "                bs = batch[\"image0\"].shape[0]\n",
    "                dumps = []\n",
    "                for b_id in range(bs):\n",
    "                    item = {}\n",
    "                    mask = batch[\"m_bids\"] == b_id\n",
    "                    item[\"pair_names\"] = pair_names[b_id]\n",
    "                    item[\"identifier\"] = \"#\".join(rel_pair_names[b_id])\n",
    "                    for key in keys_to_save:\n",
    "                        item[key] = batch[key][mask].cpu().numpy()\n",
    "                    for key in [\"R_errs\", \"t_errs\", \"inliers\"]:\n",
    "                        item[key] = batch[key][b_id]\n",
    "                    dumps.append(item)\n",
    "                ret_dict[\"dumps\"] = dumps\n",
    "\n",
    "        return ret_dict\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # metrics: dict of list, numpy\n",
    "        _metrics = [o[\"metrics\"] for o in outputs]\n",
    "        metrics = {\n",
    "            k: flattenList(gather(flattenList([_me[k] for _me in _metrics])))\n",
    "            for k in _metrics[0]\n",
    "        }\n",
    "\n",
    "        # [{key: [{...}, *#bs]}, *#batch]\n",
    "        if self.dump_dir is not None:\n",
    "            Path(self.dump_dir).mkdir(parents=True, exist_ok=True)\n",
    "            _dumps = flattenList([o[\"dumps\"] for o in outputs])  # [{...}, #bs*#batch]\n",
    "            dumps = flattenList(gather(_dumps))  # [{...}, #proc*#bs*#batch]\n",
    "            logger.info(\n",
    "                f\"Prediction and evaluation results will be saved to: {self.dump_dir}\"\n",
    "            )\n",
    "\n",
    "        if self.trainer.global_rank == 0:\n",
    "            print(self.profiler.summary())\n",
    "            val_metrics_4tb = aggregate_metrics(\n",
    "                metrics, self.config.TRAINER.EPI_ERR_THR\n",
    "            )\n",
    "            logger.info(\"\\n\" + pprint.pformat(val_metrics_4tb))\n",
    "            if self.dump_dir is not None:\n",
    "                np.save(Path(self.dump_dir) / \"LoFTR_pred_eval\", dumps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def read_megadepth_depth(path, pad_to=None):\n",
    "    depth = cv2.imread(path, 0)\n",
    "    if pad_to is not None:\n",
    "        depth, _ = pad_bottom_right(depth, pad_to, ret_mask=False)\n",
    "    depth = torch.from_numpy(depth).float()  # (h, w)\n",
    "    gc.collect()\n",
    "    return depth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "from src.utils.dataset import pad_bottom_right, read_megadepth_gray\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MegaDepthDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        npz_path,\n",
    "        mode=\"train\",\n",
    "        min_overlap_score=0.4,\n",
    "        img_resize=None,\n",
    "        df=None,\n",
    "        img_padding=False,\n",
    "        depth_padding=False,\n",
    "        augment_fn=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Manage one scene(npz_path) of MegaDepth dataset.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): megadepth root directory that has `phoenix`.\n",
    "            npz_path (str): {scene_id}.npz path. This contains image pair information of a scene.\n",
    "            mode (str): options are ['train', 'val', 'test']\n",
    "            min_overlap_score (float): how much a pair should have in common. In range of [0, 1]. Set to 0 when testing.\n",
    "            img_resize (int, optional): the longer edge of resized images. None for no resize. 640 is recommended.\n",
    "                                        This is useful during training with batches and testing with memory intensive algorithms.\n",
    "            df (int, optional): image size division factor. NOTE: this will change the final image size after img_resize.\n",
    "            img_padding (bool): If set to 'True', zero-pad the image to squared size. This is useful during training.\n",
    "            depth_padding (bool): If set to 'True', zero-pad depthmap to (2000, 2000). This is useful during training.\n",
    "            augment_fn (callable, optional): augments images with pre-defined visual effects.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "\n",
    "        # prepare scene_info and pair_info\n",
    "        if mode == \"test\" and min_overlap_score != 0:\n",
    "            logger.warning(\n",
    "                \"You are using `min_overlap_score`!=0 in test mode. Set to 0.\"\n",
    "            )\n",
    "            min_overlap_score = 0\n",
    "\n",
    "        # parameters for image resizing, padding and depthmap padding\n",
    "        if mode == \"train\":\n",
    "            assert img_resize is not None and img_padding and depth_padding\n",
    "        self.img_resize = img_resize\n",
    "        self.df = df\n",
    "        self.img_padding = img_padding\n",
    "        self.depth_max_size = (\n",
    "            2000 if depth_padding else None\n",
    "        )  # the upperbound of depthmaps size in megadepth.\n",
    "\n",
    "        # for training LoFTR\n",
    "        self.augment_fn = augment_fn if mode == \"train\" else None\n",
    "        self.coarse_scale = getattr(kwargs, \"coarse_scale\", 0.125)\n",
    "        self.path1 = data[\"path1\"].values\n",
    "        self.path2 = data[\"path2\"].values\n",
    "        self.camerainst1 = data[\"camerainst1\"].values\n",
    "        self.camerainst2 = data[\"camerainst2\"].values\n",
    "        self.rot1 = data[\"rot1\"].values\n",
    "        self.rot2 = data[\"rot2\"].values\n",
    "        self.trans1 = data[\"trans1\"].values\n",
    "        self.trans2 = data[\"trans2\"].values\n",
    "        gc.collect()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read grayscale image and mask. (1, h, w) and (h, w)\n",
    "        img_name0 = self.path1[idx]\n",
    "        img_name1 = self.path2[idx]\n",
    "\n",
    "        # TODO: Support augmentation & handle seeds for each worker correctly.\n",
    "        image0, mask0, scale0 = read_megadepth_gray(\n",
    "            img_name0, self.img_resize, self.df, self.img_padding, None\n",
    "        )\n",
    "        # np.random.choice([self.augment_fn, None], p=[0.5, 0.5]))\n",
    "        image1, mask1, scale1 = read_megadepth_gray(\n",
    "            img_name1, self.img_resize, self.df, self.img_padding, None\n",
    "        )\n",
    "        # np.random.choice([self.augment_fn, None], p=[0.5, 0.5]))\n",
    "        depth_path0 = (\n",
    "            \"./input/generate-depth-masks/depth_maps/\"\n",
    "            + img_name0.split(\"/\")[-3]\n",
    "            + \"/\"\n",
    "            + img_name0.split(\"/\")[-1]\n",
    "        )\n",
    "        depth_path1 = (\n",
    "            \"./input/generate-depth-masks/depth_maps/\"\n",
    "            + img_name1.split(\"/\")[-3]\n",
    "            + \"/\"\n",
    "            + img_name1.split(\"/\")[-1]\n",
    "        )\n",
    "\n",
    "        # read depth. shape: (h, w)\n",
    "        if self.mode in [\"train\", \"val\"]:\n",
    "            depth0 = read_megadepth_depth(depth_path0, pad_to=self.depth_max_size)\n",
    "            depth1 = read_megadepth_depth(depth_path1, pad_to=self.depth_max_size)\n",
    "        else:\n",
    "            depth0 = depth1 = torch.tensor([])\n",
    "\n",
    "        # read intrinsics of original size\n",
    "        K_0 = torch.tensor(\n",
    "            np.asarray([float(x) for x in self.camerainst1[idx].split(\" \")]),\n",
    "            dtype=torch.float,\n",
    "        ).reshape(3, 3)\n",
    "        K_1 = torch.tensor(\n",
    "            np.asarray([float(x) for x in self.camerainst2[idx].split(\" \")]),\n",
    "            dtype=torch.float,\n",
    "        ).reshape(3, 3)\n",
    "\n",
    "        # read and compute relative poses\n",
    "        R0 = self.rot1[idx].replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")\n",
    "        R0 = np.asarray([float(x) for x in R0.split(\" \")]).reshape(3, 3)\n",
    "        Tv0 = self.trans1[idx].replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")\n",
    "        Tv0 = np.asarray([[float(x) for x in Tv0.split(\" \")]])\n",
    "        T0 = np.concatenate((R0, Tv0.T), axis=1)\n",
    "        T0 = np.concatenate((T0, np.asarray([[0, 0, 0, 1]])), axis=0)\n",
    "        del R0\n",
    "        del Tv0\n",
    "        R1 = self.rot2[idx].replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")\n",
    "        R1 = np.asarray([float(x) for x in R1.split(\" \")]).reshape(3, 3)\n",
    "        Tv1 = self.trans2[idx].replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")\n",
    "        Tv1 = np.asarray([[float(x) for x in Tv1.split(\" \")]])\n",
    "        T1 = np.concatenate((R1, Tv1.T), axis=1)\n",
    "        T1 = np.concatenate((T1, np.asarray([[0, 0, 0, 1]])), axis=0)\n",
    "        del R1\n",
    "        del Tv1\n",
    "        T_0to1 = torch.tensor(np.matmul(T1, np.linalg.inv(T0)), dtype=torch.float)[\n",
    "            :4, :4\n",
    "        ]  # (4, 4)\n",
    "        T_1to0 = T_0to1.inverse()\n",
    "\n",
    "        data = {\n",
    "            \"image0\": image0,  # (1, h, w)\n",
    "            \"depth0\": depth0,  # (h, w)\n",
    "            \"image1\": image1,\n",
    "            \"depth1\": depth1,\n",
    "            \"T_0to1\": T_0to1,  # (4, 4)\n",
    "            \"T_1to0\": T_1to0,\n",
    "            \"K0\": K_0,  # (3, 3)\n",
    "            \"K1\": K_1,\n",
    "            \"scale0\": scale0,  # [scale_w, scale_h]\n",
    "            \"scale1\": scale1,\n",
    "            \"dataset_name\": \"MegaDepth\",\n",
    "            \"scene_id\": idx,\n",
    "            \"pair_id\": idx,\n",
    "            \"pair_names\": (img_name0, img_name1),\n",
    "        }\n",
    "\n",
    "        # for LoFTR training\n",
    "        if mask0 is not None:  # img_padding is True\n",
    "            if self.coarse_scale:\n",
    "                [ts_mask_0, ts_mask_1] = F.interpolate(\n",
    "                    torch.stack([mask0, mask1], dim=0)[None].float(),\n",
    "                    scale_factor=self.coarse_scale,\n",
    "                    mode=\"nearest\",\n",
    "                    recompute_scale_factor=False,\n",
    "                )[0].bool()\n",
    "            data.update({\"mask0\": ts_mask_0, \"mask1\": ts_mask_1})\n",
    "        del image0\n",
    "        del image1\n",
    "        del depth0\n",
    "        del depth1\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import os\n",
    "from collections import abc\n",
    "from os import path as osp\n",
    "from pathlib import Path\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from joblib import Parallel, delayed\n",
    "from loguru import logger\n",
    "from src.datasets.sampler import RandomConcatSampler\n",
    "from src.utils import comm\n",
    "from src.utils.augment import build_augmentor\n",
    "from src.utils.dataloader import get_local_split\n",
    "from src.utils.misc import tqdm_joblib\n",
    "from torch import distributed as dist\n",
    "from torch.utils.data import (\n",
    "    ConcatDataset,\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    DistributedSampler,\n",
    "    RandomSampler,\n",
    "    dataloader,\n",
    ")\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MultiSceneDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    For distributed training, each training process is assgined\n",
    "    only a part of the training scenes to reduce memory overhead.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, config, data):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. data config\n",
    "        # Train and Val should from the same data source\n",
    "        self.trainval_data_source = config.DATASET.TRAINVAL_DATA_SOURCE\n",
    "        self.test_data_source = config.DATASET.TEST_DATA_SOURCE\n",
    "        # training and validating\n",
    "        self.train_data = data\n",
    "        self.train_pose_root = config.DATASET.TRAIN_POSE_ROOT  # (optional)\n",
    "        self.train_npz_root = config.DATASET.TRAIN_NPZ_ROOT\n",
    "        self.train_list_path = config.DATASET.TRAIN_LIST_PATH\n",
    "        self.train_intrinsic_path = config.DATASET.TRAIN_INTRINSIC_PATH\n",
    "        self.val_data = data\n",
    "        self.val_pose_root = config.DATASET.VAL_POSE_ROOT  # (optional)\n",
    "        self.val_npz_root = config.DATASET.VAL_NPZ_ROOT\n",
    "        self.val_list_path = config.DATASET.VAL_LIST_PATH\n",
    "        self.val_intrinsic_path = config.DATASET.VAL_INTRINSIC_PATH\n",
    "        # testing\n",
    "        self.test_data = data\n",
    "        self.test_pose_root = config.DATASET.TEST_POSE_ROOT  # (optional)\n",
    "        self.test_npz_root = config.DATASET.TEST_NPZ_ROOT\n",
    "        self.test_list_path = config.DATASET.TEST_LIST_PATH\n",
    "        self.test_intrinsic_path = config.DATASET.TEST_INTRINSIC_PATH\n",
    "\n",
    "        # 2. dataset config\n",
    "        # general options\n",
    "        self.min_overlap_score_test = (\n",
    "            config.DATASET.MIN_OVERLAP_SCORE_TEST\n",
    "        )  # 0.4, omit data with overlap_score < min_overlap_score\n",
    "        self.min_overlap_score_train = config.DATASET.MIN_OVERLAP_SCORE_TRAIN\n",
    "        self.augment_fn = build_augmentor(\n",
    "            config.DATASET.AUGMENTATION_TYPE\n",
    "        )  # None, options: [None, 'dark', 'mobile']\n",
    "\n",
    "        # MegaDepth options\n",
    "        self.mgdpt_img_resize = config.DATASET.MGDPT_IMG_RESIZE  # 840\n",
    "        self.mgdpt_img_pad = config.DATASET.MGDPT_IMG_PAD  # True\n",
    "        self.mgdpt_depth_pad = config.DATASET.MGDPT_DEPTH_PAD  # True\n",
    "        self.mgdpt_df = config.DATASET.MGDPT_DF  # 8\n",
    "        self.coarse_scale = 1 / config.LOFTR.RESOLUTION[0]  # 0.125. for training loftr.\n",
    "\n",
    "        # 3.loader parameters\n",
    "        self.train_loader_params = {\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"num_workers\": args.num_workers,\n",
    "            \"pin_memory\": getattr(args, \"pin_memory\", True),\n",
    "        }\n",
    "        self.val_loader_params = {\n",
    "            \"batch_size\": 1,\n",
    "            \"shuffle\": False,\n",
    "            \"num_workers\": args.num_workers,\n",
    "            \"pin_memory\": getattr(args, \"pin_memory\", True),\n",
    "        }\n",
    "        self.test_loader_params = {\n",
    "            \"batch_size\": 1,\n",
    "            \"shuffle\": False,\n",
    "            \"num_workers\": args.num_workers,\n",
    "            \"pin_memory\": True,\n",
    "        }\n",
    "\n",
    "        # 4. sampler\n",
    "        self.data_sampler = config.TRAINER.DATA_SAMPLER\n",
    "        self.n_samples_per_subset = config.TRAINER.N_SAMPLES_PER_SUBSET\n",
    "        self.subset_replacement = config.TRAINER.SB_SUBSET_SAMPLE_REPLACEMENT\n",
    "        self.shuffle = config.TRAINER.SB_SUBSET_SHUFFLE\n",
    "        self.repeat = config.TRAINER.SB_REPEAT\n",
    "\n",
    "        # (optional) RandomSampler for debugging\n",
    "\n",
    "        # misc configurations\n",
    "        self.parallel_load_data = getattr(args, \"parallel_load_data\", False)\n",
    "        self.seed = config.TRAINER.SEED  # 66\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Setup train / val / test dataset. This method will be called by PL automatically.\n",
    "        Args:\n",
    "            stage (str): 'fit' in training phase, and 'test' in testing phase.\n",
    "        \"\"\"\n",
    "\n",
    "        assert stage in [\"fit\", \"test\"], \"stage must be either fit or test\"\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = self._setup_dataset(\n",
    "                self.train_data,\n",
    "                self.train_npz_root,\n",
    "                self.train_list_path,\n",
    "                self.train_intrinsic_path,\n",
    "                mode=\"train\",\n",
    "                min_overlap_score=self.min_overlap_score_train,\n",
    "                pose_dir=self.train_pose_root,\n",
    "            )\n",
    "            # setup multiple (optional) validation subsets\n",
    "\n",
    "            self.val_dataset = self._setup_dataset(\n",
    "                self.val_data,\n",
    "                self.val_npz_root,\n",
    "                self.val_list_path,\n",
    "                self.val_intrinsic_path,\n",
    "                mode=\"val\",\n",
    "                min_overlap_score=self.min_overlap_score_test,\n",
    "                pose_dir=self.val_pose_root,\n",
    "            )\n",
    "\n",
    "        else:  # stage == 'test\n",
    "            self.test_dataset = self._setup_dataset(\n",
    "                self.test_data,\n",
    "                self.test_npz_root,\n",
    "                self.test_list_path,\n",
    "                self.test_intrinsic_path,\n",
    "                mode=\"test\",\n",
    "                min_overlap_score=self.min_overlap_score_test,\n",
    "                pose_dir=self.test_pose_root,\n",
    "            )\n",
    "\n",
    "    def _setup_dataset(\n",
    "        self,\n",
    "        data,\n",
    "        split_npz_root,\n",
    "        scene_list_path,\n",
    "        intri_path,\n",
    "        mode=\"train\",\n",
    "        min_overlap_score=0.0,\n",
    "        pose_dir=None,\n",
    "    ):\n",
    "        \"\"\"Setup train / val / test set\"\"\"\n",
    "        local_npz_names = \"\"\n",
    "        dataset_builder = self._build_concat_dataset\n",
    "        return dataset_builder(\n",
    "            data,\n",
    "            local_npz_names,\n",
    "            split_npz_root,\n",
    "            intri_path,\n",
    "            mode=mode,\n",
    "            min_overlap_score=min_overlap_score,\n",
    "            pose_dir=pose_dir,\n",
    "        )\n",
    "\n",
    "    def _build_concat_dataset(\n",
    "        self,\n",
    "        data,\n",
    "        npz_names,\n",
    "        npz_dir,\n",
    "        intrinsic_path,\n",
    "        mode,\n",
    "        min_overlap_score=0.0,\n",
    "        pose_dir=None,\n",
    "    ):\n",
    "        datasets = []\n",
    "        augment_fn = self.augment_fn if mode == \"train\" else None\n",
    "        data_source = (\n",
    "            self.trainval_data_source\n",
    "            if mode in [\"train\", \"val\"]\n",
    "            else self.test_data_source\n",
    "        )\n",
    "        npz_path = \"\"\n",
    "\n",
    "        datasets.append(\n",
    "            MegaDepthDataset(\n",
    "                data,\n",
    "                npz_path,\n",
    "                mode=mode,\n",
    "                min_overlap_score=min_overlap_score,\n",
    "                img_resize=self.mgdpt_img_resize,\n",
    "                df=self.mgdpt_df,\n",
    "                img_padding=self.mgdpt_img_pad,\n",
    "                depth_padding=self.mgdpt_depth_pad,\n",
    "                augment_fn=augment_fn,\n",
    "                coarse_scale=self.coarse_scale,\n",
    "            )\n",
    "        )\n",
    "        return ConcatDataset(datasets)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Build training dataloader for ScanNet / MegaDepth.\"\"\"\n",
    "        #         assert self.data_sampler in ['scene_balance']\n",
    "        #         #logger.info(f'[rank:{self.rank}/{self.world_size}]: Train Sampler and DataLoader re-init (should not re-init between epochs!).')\n",
    "        #         if self.data_sampler == 'scene_balance':\n",
    "        #             sampler = RandomConcatSampler(self.train_dataset,\n",
    "        #                                           self.n_samples_per_subset,\n",
    "        #                                           self.subset_replacement,\n",
    "        #                                           self.shuffle, self.repeat, self.seed)\n",
    "        #         else:\n",
    "        #             sampler = None\n",
    "        dataloader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Build validation dataloader for ScanNet / MegaDepth.\"\"\"\n",
    "        # logger.info(f'[rank:{self.rank}/{self.world_size}]: Val Sampler and DataLoader re-init.')\n",
    "        dataloader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs):\n",
    "        # logger.info(f'[rank:{self.rank}/{self.world_size}]: Test Sampler and DataLoader re-init.')\n",
    "        sampler = DistributedSampler(self.test_dataset, shuffle=False)\n",
    "        return DataLoader(self.test_dataset, sampler=sampler, **self.test_loader_params)\n",
    "\n",
    "\n",
    "def _build_dataset(dataset: Dataset, *args, **kwargs):\n",
    "    return dataset(*args, **kwargs)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:07:47.164502Z",
     "iopub.execute_input": "2022-04-17T18:07:47.164761Z",
     "iopub.status.idle": "2022-04-17T18:07:48.814359Z",
     "shell.execute_reply.started": "2022-04-17T18:07:47.164728Z",
     "shell.execute_reply": "2022-04-17T18:07:48.813564Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import math\n",
    "import pprint\n",
    "from distutils.util import strtobool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from loguru import logger as loguru_logger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from src.config.default import get_cfg_defaults\n",
    "from src.utils.misc import get_rank_zero_only_logger, setup_gpus\n",
    "from src.utils.profiler import build_profiler\n",
    "\n",
    "loguru_logger = get_rank_zero_only_logger(loguru_logger)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # init a costum parser which will be added into pl.Trainer parser\n",
    "    # check documentation: https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\"data_cfg_path\", type=str, help=\"data config path\")\n",
    "    parser.add_argument(\"main_cfg_path\", type=str, help=\"main config path\")\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"default_exp_name\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch_size per gpu\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "    parser.add_argument(\n",
    "        \"--pin_memory\",\n",
    "        type=lambda x: bool(strtobool(x)),\n",
    "        nargs=\"?\",\n",
    "        default=True,\n",
    "        help=\"whether loading data to pinned memory or not\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ckpt_path\",\n",
    "        type=str,\n",
    "        default=\"./input/kornia-loftr/outdoor_ds.ckpt\",\n",
    "        help=\"pretrained checkpoint path, helpful for using a pre-trained coarse-only LoFTR\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--disable_ckpt\",\n",
    "        action=\"store_true\",\n",
    "        help=\"disable checkpoint saving (useful for debugging).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--profiler_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"options: [inference, pytorch], or leave it unset\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--parallel_load_data\",\n",
    "        action=\"store_true\",\n",
    "        help=\"load datasets in with multiple processes.\",\n",
    "    )\n",
    "\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    return parser.parse_args(\n",
    "        \"./input/loftrutils/LoFTR-master/LoFTR-master/configs/data/megadepth_trainval_640.py ./input/loftrutils/LoFTR-master/LoFTR-master/configs/loftr/outdoor/loftr_ds_dense.py --exp_name test --gpus 0 --num_nodes 0 --accelerator gpu --batch_size 1 --check_val_every_n_epoch 1 --log_every_n_steps 1 --flush_logs_every_n_steps 1 --limit_val_batches 1 --num_sanity_val_steps 10 --benchmark True --max_epochs 4\".split()\n",
    "    )\n",
    "\n",
    "\n",
    "def train():\n",
    "    # parse arguments\n",
    "    args = parse_args()\n",
    "    rank_zero_only(pprint.pprint)(vars(args))\n",
    "\n",
    "    # init default-cfg and merge it with the main- and data-cfg\n",
    "    config = get_cfg_defaults()\n",
    "    config.merge_from_file(args.main_cfg_path)\n",
    "    config.merge_from_file(args.data_cfg_path)\n",
    "    pl.seed_everything(config.TRAINER.SEED)  # reproducibility\n",
    "    # TODO: Use different seeds for each dataloader workers\n",
    "    # This is needed for data augmentation\n",
    "\n",
    "    # scale lr and warmup-step automatically\n",
    "    args.gpus = _n_gpus = setup_gpus(args.gpus)\n",
    "    config.TRAINER.WORLD_SIZE = _n_gpus * args.num_nodes\n",
    "    config.TRAINER.TRUE_BATCH_SIZE = config.TRAINER.WORLD_SIZE * args.batch_size\n",
    "    _scaling = 1  # config.TRAINER.TRUE_BATCH_SIZE / config.TRAINER.CANONICAL_BS\n",
    "    config.TRAINER.SCALING = _scaling\n",
    "    config.TRAINER.TRUE_LR = 0.00001 * _scaling\n",
    "    config.TRAINER.WARMUP_STEP = math.floor(config.TRAINER.WARMUP_STEP / _scaling)\n",
    "\n",
    "    # lightning module\n",
    "    profiler = build_profiler(args.profiler_name)\n",
    "    model = PL_LoFTR(config, pretrained_ckpt=args.ckpt_path, profiler=profiler)\n",
    "    loguru_logger.info(f\"LoFTR LightningModule initialized!\")\n",
    "\n",
    "    # lightning data\n",
    "    data = pd.read_csv(\"./input/imc-gt/train.csv\")\n",
    "    data_module = MultiSceneDataModule(args, config, data[:100])\n",
    "    gc.collect()\n",
    "    loguru_logger.info(f\"LoFTR DataModule initialized!\")\n",
    "\n",
    "    # TensorBoard Logger\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=\"logs/tb_logs\", name=args.exp_name, default_hp_metric=False\n",
    "    )\n",
    "    ckpt_dir = Path(logger.log_dir) / \"checkpoints\"\n",
    "\n",
    "    # Callbacks\n",
    "    # TODO: update ModelCheckpoint to monitor multiple metrics\n",
    "    ckpt_callback = ModelCheckpoint(\n",
    "        monitor=\"auc@10\",\n",
    "        verbose=True,\n",
    "        save_top_k=5,\n",
    "        mode=\"max\",\n",
    "        save_last=True,\n",
    "        dirpath=str(ckpt_dir),\n",
    "        filename=\"{epoch}-{auc@5:.3f}-{auc@10:.3f}-{auc@20:.3f}\",\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    callbacks = [lr_monitor]\n",
    "    if not args.disable_ckpt:\n",
    "        callbacks.append(ckpt_callback)\n",
    "\n",
    "    # Lightning Trainer\n",
    "    trainer = pl.Trainer.from_argparse_args(\n",
    "        args,\n",
    "        #         plugins=DDPPlugin(find_unused_parameters=False,\n",
    "        #                           num_nodes=args.num_nodes,\n",
    "        #                           sync_batchnorm=config.TRAINER.WORLD_SIZE > 0),\n",
    "        gradient_clip_val=config.TRAINER.GRADIENT_CLIPPING,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        # sync_batchnorm=config.TRAINER.WORLD_SIZE > 0,\n",
    "        replace_sampler_ddp=False,  # use custom sampler\n",
    "        # avoid repeated samples!\n",
    "        weights_summary=\"full\",\n",
    "        profiler=profiler,\n",
    "    )\n",
    "    loguru_logger.info(f\"Trainer initialized!\")\n",
    "    loguru_logger.info(f\"Start training!\")\n",
    "    trainer.fit(model, datamodule=data_module)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:07:48.815847Z",
     "iopub.execute_input": "2022-04-17T18:07:48.816121Z",
     "iopub.status.idle": "2022-04-17T18:07:48.849851Z",
     "shell.execute_reply.started": "2022-04-17T18:07:48.816083Z",
     "shell.execute_reply": "2022-04-17T18:07:48.849185Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-17T18:07:48.85116Z",
     "iopub.execute_input": "2022-04-17T18:07:48.851428Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accelerator': 'gpu',\n",
      " 'accumulate_grad_batches': None,\n",
      " 'amp_backend': 'native',\n",
      " 'amp_level': None,\n",
      " 'auto_lr_find': False,\n",
      " 'auto_scale_batch_size': False,\n",
      " 'auto_select_gpus': False,\n",
      " 'batch_size': 1,\n",
      " 'benchmark': True,\n",
      " 'check_val_every_n_epoch': 1,\n",
      " 'checkpoint_callback': None,\n",
      " 'ckpt_path': './input/kornia-loftr/outdoor_ds.ckpt',\n",
      " 'data_cfg_path': './input/loftrutils/LoFTR-master/LoFTR-master/configs/data/megadepth_trainval_640.py',\n",
      " 'default_root_dir': None,\n",
      " 'detect_anomaly': False,\n",
      " 'deterministic': False,\n",
      " 'devices': None,\n",
      " 'disable_ckpt': False,\n",
      " 'enable_checkpointing': True,\n",
      " 'enable_model_summary': True,\n",
      " 'enable_progress_bar': True,\n",
      " 'exp_name': 'test',\n",
      " 'fast_dev_run': False,\n",
      " 'flush_logs_every_n_steps': 1,\n",
      " 'gpus': 0,\n",
      " 'gradient_clip_algorithm': None,\n",
      " 'gradient_clip_val': None,\n",
      " 'ipus': None,\n",
      " 'limit_predict_batches': None,\n",
      " 'limit_test_batches': None,\n",
      " 'limit_train_batches': None,\n",
      " 'limit_val_batches': 1,\n",
      " 'log_every_n_steps': 1,\n",
      " 'log_gpu_memory': None,\n",
      " 'logger': True,\n",
      " 'main_cfg_path': './input/loftrutils/LoFTR-master/LoFTR-master/configs/loftr/outdoor/loftr_ds_dense.py',\n",
      " 'max_epochs': 4,\n",
      " 'max_steps': -1,\n",
      " 'max_time': None,\n",
      " 'min_epochs': None,\n",
      " 'min_steps': None,\n",
      " 'move_metrics_to_cpu': False,\n",
      " 'multiple_trainloader_mode': 'max_size_cycle',\n",
      " 'num_nodes': 0,\n",
      " 'num_processes': None,\n",
      " 'num_sanity_val_steps': 10,\n",
      " 'num_workers': 4,\n",
      " 'overfit_batches': 0.0,\n",
      " 'parallel_load_data': False,\n",
      " 'pin_memory': True,\n",
      " 'plugins': None,\n",
      " 'precision': 32,\n",
      " 'prepare_data_per_node': None,\n",
      " 'process_position': 0,\n",
      " 'profiler': None,\n",
      " 'profiler_name': None,\n",
      " 'progress_bar_refresh_rate': None,\n",
      " 'reload_dataloaders_every_n_epochs': 0,\n",
      " 'replace_sampler_ddp': True,\n",
      " 'resume_from_checkpoint': None,\n",
      " 'stochastic_weight_avg': False,\n",
      " 'strategy': None,\n",
      " 'sync_batchnorm': False,\n",
      " 'terminate_on_nan': None,\n",
      " 'tpu_cores': None,\n",
      " 'track_grad_norm': -1,\n",
      " 'val_check_interval': None,\n",
      " 'weights_save_path': None,\n",
      " 'weights_summary': 'top'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [27]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;66;03m# lightning module\u001B[39;00m\n\u001B[1;32m     91\u001B[0m profiler \u001B[38;5;241m=\u001B[39m build_profiler(args\u001B[38;5;241m.\u001B[39mprofiler_name)\n\u001B[0;32m---> 92\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mPL_LoFTR\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_ckpt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofiler\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprofiler\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m loguru_logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoFTR LightningModule initialized!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# lightning data\u001B[39;00m\n",
      "Input \u001B[0;32mIn [23]\u001B[0m, in \u001B[0;36mPL_LoFTR.__init__\u001B[0;34m(self, config, pretrained_ckpt, profiler, dump_dir)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Pretrained weights\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pretrained_ckpt:\n\u001B[0;32m---> 23\u001B[0m     state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_ckpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmatcher\u001B[38;5;241m.\u001B[39mload_state_dict(state_dict, strict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     25\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoad \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_ckpt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m as pretrained checkpoint\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:712\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[1;32m    710\u001B[0m             opened_file\u001B[38;5;241m.\u001B[39mseek(orig_position)\n\u001B[1;32m    711\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mload(opened_file)\n\u001B[0;32m--> 712\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:1046\u001B[0m, in \u001B[0;36m_load\u001B[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1044\u001B[0m unpickler \u001B[38;5;241m=\u001B[39m UnpicklerWrapper(data_file, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[1;32m   1045\u001B[0m unpickler\u001B[38;5;241m.\u001B[39mpersistent_load \u001B[38;5;241m=\u001B[39m persistent_load\n\u001B[0;32m-> 1046\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1048\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_validate_loaded_sparse_tensors()\n\u001B[1;32m   1050\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:1016\u001B[0m, in \u001B[0;36m_load.<locals>.persistent_load\u001B[0;34m(saved_id)\u001B[0m\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m loaded_storages:\n\u001B[1;32m   1015\u001B[0m     nbytes \u001B[38;5;241m=\u001B[39m numel \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_element_size(dtype)\n\u001B[0;32m-> 1016\u001B[0m     \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loaded_storages[key]\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:1001\u001B[0m, in \u001B[0;36m_load.<locals>.load_tensor\u001B[0;34m(dtype, numel, key, location)\u001B[0m\n\u001B[1;32m    997\u001B[0m storage \u001B[38;5;241m=\u001B[39m zip_file\u001B[38;5;241m.\u001B[39mget_storage_from_record(name, numel, torch\u001B[38;5;241m.\u001B[39m_UntypedStorage)\u001B[38;5;241m.\u001B[39mstorage()\u001B[38;5;241m.\u001B[39m_untyped()\n\u001B[1;32m    998\u001B[0m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[1;32m    999\u001B[0m \u001B[38;5;66;03m# stop wrapping with _TypedStorage\u001B[39;00m\n\u001B[1;32m   1000\u001B[0m loaded_storages[key] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39m_TypedStorage(\n\u001B[0;32m-> 1001\u001B[0m     wrap_storage\u001B[38;5;241m=\u001B[39m\u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1002\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:970\u001B[0m, in \u001B[0;36m_get_restore_location.<locals>.restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrestore_location\u001B[39m(storage, location):\n\u001B[0;32m--> 970\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdefault_restore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:176\u001B[0m, in \u001B[0;36mdefault_restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_restore_location\u001B[39m(storage, location):\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[0;32m--> 176\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    178\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:152\u001B[0m, in \u001B[0;36m_cuda_deserialize\u001B[0;34m(obj, location)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_cuda_deserialize\u001B[39m(obj, location):\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m location\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 152\u001B[0m         device \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate_cuda_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    153\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_torch_load_uninitialized\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    154\u001B[0m             storage_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39mcuda, \u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/image-matching-challenge-2022/.venv/lib/python3.8/site-packages/torch/serialization.py:136\u001B[0m, in \u001B[0;36mvalidate_cuda_device\u001B[0;34m(location)\u001B[0m\n\u001B[1;32m    133\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_get_device_index(location, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available():\n\u001B[0;32m--> 136\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAttempting to deserialize object on a CUDA \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    137\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice but torch.cuda.is_available() is False. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    138\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIf you are running on a CPU-only machine, \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    139\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mplease use torch.load with map_location=torch.device(\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    140\u001B[0m                        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mto map your storages to the CPU.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    141\u001B[0m device_count \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice_count()\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m device_count:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd checkpoints/"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ls"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
